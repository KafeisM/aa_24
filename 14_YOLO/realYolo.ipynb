{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac535cf-32e4-4c01-a6a2-ffc82f3828ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb67056-ac43-41fb-9c4a-ac5168ad822b",
   "metadata": {},
   "source": [
    "# YOLO \n",
    "\n",
    "En aquesta sessió descobrirem un dels models de l'estat de l'art (SOTA) de l'Aprenentatge Automàtic.\n",
    "\n",
    "You Only Look Once: Unified, Real-Time Object Detection. [article](https://arxiv.org/pdf/1506.02640) ; [presentació](https://www.youtube.com/watch?v=NM6lrxy0bxs&pp=ygUkeW91IG9ubHkgbG9vayBvbmNlIHByZXNlbnRhdGlvbiBjdnBy)\n",
    "\n",
    "YOLO (You Only Look Once) és una arquitectura de xarxa neuronal profunda inicialment dissenyada per a la detecció d'objectes en imatges en temps real. A diferència d'altres enfocaments que processen les imatges en diverses etapes com per exemple la família R-CNN. YOLO adopta un enfocament unificat: divideix la imatge en una quadrícula i processa cada cel·la simultàniament per predir les bounding boxes i les classes dels objectes presents. Aquesta integració permet assolir una velocitat notable sense comprometre significativament la precisió. A més, gràcies al seu disseny optimitzat, YOLO ha estat modificada i adaptada per realitzar múltiples tasques relacionades amb la visió per computador. Actualment pot: classificar, detectar, segmentar, seguiment d'objectes a vídeo, seguiment dels moviments del cos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bd7aa-cae8-45ce-8d96-fb3163106f57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### YOLO: Una breu història\n",
    "\n",
    "- **YOLO (You Only Look Once)**, un model popular de detecció d'objectes i segmentació d'imatges, va ser desenvolupat per Joseph Redmon i Ali Farhadi a la Universitat de Washington. Llançat el 2015, YOLO va guanyar ràpidament popularitat per la seva alta velocitat i precisió.\n",
    "- **YOLOv2**, llançat el 2016, va millorar el model original incorporant _batch normalization_ i  _anchor boxes_.\n",
    "- **YOLOv3**, llançat el 2018, va millorar encara més el rendiment del model mitjançant un _backbone_ més eficient, múltiples  _anchor boxes_ i agrupació de piràmides espacials (objectes de múltiples mides).\n",
    "- **YOLOv4** es va llançar el 2020, introduint innovacions com l'augment de dades emprant mosaics, un nou capçal (_head_) de detecció i una nova funció de pèrdua.\n",
    "- **YOLOv5** va millorar encara més el rendiment del model i va afegir noves funcions com ara l'optimització d'hiperparàmetres, el seguiment d'experiments integrat i l'exportació automàtica a formats d'exportació populars. [**Controvèrsia!!**](https://blog.roboflow.com/yolov4-versus-yolov5/)\n",
    "- **YOLOv6** va ser de codi obert per [Meituan](https://github.com/meituan/YOLOv6) el 2022 i s'utilitza en molts dels robots de lliurament autònoms de la companyia.\n",
    "- **YOLOv7** va afegir tasques addicionals, com ara l'estimació de poses (_pose estimation_) al conjunt de dades de punts clau COCO.\n",
    "- **YOLOv8** Es basa en l'èxit de les versions anteriors, introduint noves funcions i millores per millorar el rendiment, la flexibilitat i l'eficiència. YOLOv8 admet una gamma completa de tasques d'IA de visió, com ara detecció, segmentació, estimació de poses, seguiment i classificació. Aquesta versatilitat permet als usuaris aprofitar les capacitats de YOLOv8 en diferents aplicacions i dominis. --> Propaganda Ultralytics ^^.\n",
    "- **YOLOv9**, **YOLOv10**, **YOLOv11** : Successives millores en la xarxa, sobretot enfocades a mantenir les seves capacitats reduïnt la seva mida.\n",
    "\n",
    "Per començar i fer les primeres proves es recomana usar la versió 5, a que té un bon equilibri entre dificultat (complicacions d'ús) i els resultats que podem obtenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19f761-f55c-4010-b799-930925b8e90f",
   "metadata": {},
   "source": [
    "## Arquitectura\n",
    "\n",
    "La xarxa té 24 capes convolucionals seguides de 2 capes completament connectades. En lloc dels mòduls inicials utilitzats per [GoogLeNet](https://arxiv.org/pdf/1409.4842), per reduir el mapes d'activació s'utilitzen capes convolucionals 1×1 seguides de capes convolucionals de 3×3.\n",
    "\n",
    "![YOLO](img/YOLO.png \"YOLO\")\n",
    "\n",
    "\n",
    "### Detecció unificada\n",
    "\n",
    "A diferència de les xarxes que provenen de R-CNN a l'article s'explica:\n",
    "\n",
    "> Unifiquem els components separats de la detecció d'objectes en una única xarxa neuronal. La nostra xarxa utilitza funcions\n",
    "de tota la imatge per predir cada quadre delimitador. També prediu tots els quadres delimitadors (_bounding boxes_) de totes les classes per a una imatge simultàniament. Això vol dir que la nostra xarxa raona globalment sobre la imatge completa i tots els objectes de la imatge.\n",
    "\n",
    "El sistema YOLO divideix la imatge d'entrada en una graella $S×S$. Si el centre d'un objecte cau dins d'una cel·la de quadrícula, aquesta cel·la de quadrícula s'encarrega de detectar aquest objecte. Cada cel·la de la quadrícula prediu $B$ quadres de delimitació i els valors de confiança per a aquestes caixes. \n",
    "\n",
    "Aquestes puntuacions de confiança reflecteixen la confiança que té el model que la caixa conté un objecte i també la precisió que creu que és la caixa que prediu. Cada quadre delimitador consta de 5 prediccions: $x, y, w, h$ i la confiança. Les coordenades $(x, y)$ representen el centre del quadre en relació amb els límits de la cel·la de la quadrícula. L'amplada i l'alçada es prediuen en relació amb tota la imatge. Finalment, la predicció de confiança representa l'IOU entre la caixa predita i qualsevol caixa de veritat terrestre. Cada cel·la de la quadrícula també prediu probabilitats de classe condicional $C$.\n",
    "\n",
    "\n",
    "![YOLO](img/YOLO_deteccio.png \"YOLO\")\n",
    "\n",
    "L'arquitectura que hem mostrat anteriorment té una capa de sortida de $7x7x30$ degut a que  la sortida és correspon amb la següentv fórmula: $S × S × (B ∗ 5 + C)$ en el cas de l'article original: $S=7$, $B=2$ i $C=20$ ja que es va entrenar amb el dataset [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n",
    "\n",
    "Actualment l'arquitectura de la xarxa és molt més complexa, i inclou 3 blocs molt diferenciats:\n",
    "\n",
    "- **Columna** \"Backbone\": Bàsicament és una xarxa convolucional que extreu característiques. A partir de la versió 3 creen la seva pròpia xarxa anomenada DarkNet, un model amb connexions residuals que té al voltant de 53 capes.\n",
    "- **Coll**: Aquesta part connecta la columna i el/els caps. S'encarrega entre d'altres coses de la detecció d'objectes a múltiples escales mitjançant xarxes piramidals que reben informació de diversos punts del \"Backbone.\n",
    "- **Cap**: El cap/caps s'encarrega de fer prediccions. En les versions modernes de YOLO s'utilitzen múltiples mòduls de detecció que prediuen quadres delimitadors, puntuacions d'objectivitat i probabilitats de classe per a cada cel·la de quadrícula del mapa de característiques. A continuació, aquestes prediccions s'agreguen per obtenir les deteccions finals.\n",
    "\n",
    "Podem veure un exemple d'aquesta complexa arquitectura en el següent enllaç a la documentació oficial de la [YOLO v5](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/#1-model-structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3b32c-07ee-4aa3-918d-99807c171dae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ús de la xarxa\n",
    "\n",
    "La manera més senzilla d'emprar la xarxa és desde la llibreria que la empresa Ultralytics ens ofereix. D'aquesta manera ens és molt senzill poder provar les diferents versions de la xarxa i també realitzar els processos de _fine tunning_ o de _transfer learning_."
   ]
  },
  {
   "cell_type": "code",
   "id": "86c442c9-a9a9-475f-84d4-b44d88c2480b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T15:53:36.365586Z",
     "start_time": "2024-11-27T15:53:32.210072Z"
    }
   },
   "source": "!pip install -U ultralytics",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (8.3.38)\r\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (1.26.4)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (3.9.2)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (10.4.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (6.0.2)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (2.32.3)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (2.5.1)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (0.15.2a0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (4.66.5)\r\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (5.9.0)\r\n",
      "Requirement already satisfied: py-cpuinfo in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (9.0.0)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (2.2.2)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (0.13.2)\r\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\r\n",
      "  Downloading ultralytics_thop-2.0.12-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\r\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\r\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\r\n",
      "Collecting fsspec (from torch>=1.8.0->ultralytics)\r\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting sympy==1.13.1 (from torch>=1.8.0->ultralytics)\r\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\r\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\r\n",
      "Downloading ultralytics_thop-2.0.12-py3-none-any.whl (26 kB)\r\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\r\n",
      "Installing collected packages: sympy, fsspec, ultralytics-thop\r\n",
      "  Attempting uninstall: sympy\r\n",
      "    Found existing installation: sympy 1.13.2\r\n",
      "    Uninstalling sympy-1.13.2:\r\n",
      "      Successfully uninstalled sympy-1.13.2\r\n",
      "Successfully installed fsspec-2024.10.0 sympy-1.13.1 ultralytics-thop-2.0.12\r\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "c6337fa5-33c5-4ed8-92d2-248032570989",
   "metadata": {},
   "source": [
    "Començarem fent proves amb la YoloV5 que presenta 5 versions diferents, cada una d'aquestes versions té una xarxa _backbone_ de mida diferent, a més tenim 2 mides d'entrada d'imatges:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>size<br><sup>(pixels)</sup></th>\n",
    "      <th>mAP<sup>val<br>50-95</sup></th>\n",
    "      <th>mAP<sup>val<br>50</sup></th>\n",
    "      <th>Speed<br><sup>CPU b1<br>(ms)</sup></th>\n",
    "      <th>Speed<br><sup>V100 b1<br>(ms)</sup></th>\n",
    "      <th>Speed<br><sup>V100 b32<br>(ms)</sup></th>\n",
    "      <th>params<br><sup>(M)</sup></th>\n",
    "      <th>FLOPs<br><sup>@640 (B)</sup></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt\" target=\"_blank\">YOLOv5n</a></td>\n",
    "      <td>640</td>\n",
    "      <td>28.0</td>\n",
    "      <td>45.7</td>\n",
    "      <td><strong>45</strong></td>\n",
    "      <td><strong>6.3</strong></td>\n",
    "      <td><strong>0.6</strong></td>\n",
    "      <td><strong>1.9</strong></td>\n",
    "      <td><strong>4.5</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\" target=\"_blank\">YOLOv5s</a></td>\n",
    "      <td>640</td>\n",
    "      <td>37.4</td>\n",
    "      <td>56.8</td>\n",
    "      <td>98</td>\n",
    "      <td>6.4</td>\n",
    "      <td>0.9</td>\n",
    "      <td>7.2</td>\n",
    "      <td>16.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt\" target=\"_blank\">YOLOv5m</a></td>\n",
    "      <td>640</td>\n",
    "      <td>45.4</td>\n",
    "      <td>64.1</td>\n",
    "      <td>224</td>\n",
    "      <td>8.2</td>\n",
    "      <td>1.7</td>\n",
    "      <td>21.2</td>\n",
    "      <td>49.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt\" target=\"_blank\">YOLOv5l</a></td>\n",
    "      <td>640</td>\n",
    "      <td>49.0</td>\n",
    "      <td>67.3</td>\n",
    "      <td>430</td>\n",
    "      <td>10.1</td>\n",
    "      <td>2.7</td>\n",
    "      <td>46.5</td>\n",
    "      <td>109.1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt\" target=\"_blank\">YOLOv5x</a></td>\n",
    "      <td>640</td>\n",
    "      <td>50.7</td>\n",
    "      <td>68.9</td>\n",
    "      <td>766</td>\n",
    "      <td>12.1</td>\n",
    "      <td>4.8</td>\n",
    "      <td>86.7</td>\n",
    "      <td>205.7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt\" target=\"_blank\">YOLOv5n6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>36.0</td>\n",
    "      <td>54.4</td>\n",
    "      <td>153</td>\n",
    "      <td>8.1</td>\n",
    "      <td>2.1</td>\n",
    "      <td>3.2</td>\n",
    "      <td>4.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt\" target=\"_blank\">YOLOv5s6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>44.8</td>\n",
    "      <td>63.7</td>\n",
    "      <td>385</td>\n",
    "      <td>8.2</td>\n",
    "      <td>3.6</td>\n",
    "      <td>12.6</td>\n",
    "      <td>16.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt\" target=\"_blank\">YOLOv5m6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>51.3</td>\n",
    "      <td>69.3</td>\n",
    "      <td>887</td>\n",
    "      <td>11.1</td>\n",
    "      <td>6.8</td>\n",
    "      <td>35.7</td>\n",
    "      <td>50.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l6.pt\" target=\"_blank\">YOLOv5l6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>53.7</td>\n",
    "      <td>71.3</td>\n",
    "      <td>1784</td>\n",
    "      <td>15.8</td>\n",
    "      <td>10.5</td>\n",
    "      <td>76.8</td>\n",
    "      <td>111.4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x6.pt\" target=\"_blank\">YOLOv5x6</a><br>+ [TTA]</td>\n",
    "      <td>1280<br>1536</td>\n",
    "      <td>55.0<br><strong>55.8</strong></td>\n",
    "      <td>72.7<br><strong>72.7</strong></td>\n",
    "      <td>3136<br>-</td>\n",
    "      <td>26.2<br>-</td>\n",
    "      <td>19.4<br>-</td>\n",
    "      <td>140.7<br>-</td>\n",
    "      <td>209.8<br>-</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Explicació mètrica [mAP](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)\n",
    "\n",
    "Nosaltres començarem fent proves amb la versió més petita:"
   ]
  },
  {
   "cell_type": "code",
   "id": "8f0f76f9-1c2c-496c-97b8-b5bbebff4ac2",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T16:02:20.667306Z",
     "start_time": "2024-11-27T16:02:20.641916Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv5n model\n",
    "model = YOLO(\"yolov5n.pt\")\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP 💡 Replace 'model=yolov5n.pt' with new 'model=yolov5nu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "YOLOv5n summary: 262 layers, 2,654,816 parameters, 0 gradients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(262, 2654816, 0, 0.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "12111eb9-3ae8-4b54-b0bf-8cdd5aacd8df",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T16:02:22.932528Z",
     "start_time": "2024-11-27T16:02:22.929830Z"
    }
   },
   "source": [
    "model;"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "980eac5e-e39f-4ebe-8389-d6869976fbfc",
   "metadata": {},
   "source": [
    "### Inferència\n",
    "\n",
    "YOLOv5 ha estat entrenat amb el dataset COCO (Common Objects in COntext) [enllaç](https://cocodataset.org/#home) que en té 80 classes diferents. Fer la inferència per detecció és molt senzill, és suficient amb cridar al model. Aquest ens retorna un objecte de tipus _Results_. [Documentació](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Results).\n",
    "\n",
    "Així, el procés d'inferència empra l'API d'Ultralytics i es fa enfora de _Pytorch_. "
   ]
  },
  {
   "cell_type": "code",
   "id": "ac12e4c3-36ba-4812-af92-7c543bba6710",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T16:02:24.442304Z",
     "start_time": "2024-11-27T16:02:24.439834Z"
    }
   },
   "source": [
    "# Accepta URL, path, PIL, OpenCV, numpy o una llista\n",
    "img = \"https://hips.hearstapps.com/hmg-prod/images/the-boys-serie-amazon-1565605836.jpg\""
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "16543479-afdc-41e8-888c-197a5233e2cd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T16:02:26.012739Z",
     "start_time": "2024-11-27T16:02:25.907743Z"
    }
   },
   "source": [
    "# Inferencia\n",
    "results = model(img)\n",
    "results; # És una llista de Results"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https://hips.hearstapps.com/hmg-prod/images/the-boys-serie-amazon-1565605836.jpg locally at the-boys-serie-amazon-1565605836.jpg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Inferencia\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m results\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/engine/model.py:176\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, source, stream, **kwargs)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    149\u001B[0m     source: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28mint\u001B[39m, Image\u001B[38;5;241m.\u001B[39mImage, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray, torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    150\u001B[0m     stream: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    152\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    153\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001B[39;00m\n\u001B[1;32m    155\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001B[39;00m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 176\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/engine/model.py:554\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, source, stream, predictor, **kwargs)\u001B[0m\n\u001B[1;32m    552\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset_prompts\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mset_prompts(prompts)\n\u001B[0;32m--> 554\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mpredict_cli(source\u001B[38;5;241m=\u001B[39msource) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/engine/predictor.py:173\u001B[0m, in \u001B[0;36mBasePredictor.__call__\u001B[0;34m(self, source, model, stream, *args, **kwargs)\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/torch/utils/_contextlib.py:36\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m---> 36\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     40\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/engine/predictor.py:266\u001B[0m, in \u001B[0;36mBasePredictor.stream_inference\u001B[0;34m(self, source, model, *args, **kwargs)\u001B[0m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# Postprocess\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m profilers[\u001B[38;5;241m2\u001B[39m]:\n\u001B[0;32m--> 266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpostprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mim0s\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_callbacks(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_predict_postprocess_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    269\u001B[0m \u001B[38;5;66;03m# Visualize, save, write results\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/models/yolo/detect/predict.py:25\u001B[0m, in \u001B[0;36mDetectionPredictor.postprocess\u001B[0;34m(self, preds, img, orig_imgs)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpostprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, preds, img, orig_imgs):\n\u001B[1;32m     24\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Post-processes predictions and returns a list of Results objects.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m     preds \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnon_max_suppression\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miou\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43magnostic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magnostic_nms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_det\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_det\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(orig_imgs, \u001B[38;5;28mlist\u001B[39m):  \u001B[38;5;66;03m# input images are a torch.Tensor, not a list\u001B[39;00m\n\u001B[1;32m     35\u001B[0m         orig_imgs \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mconvert_torch2numpy_batch(orig_imgs)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/utils/ops.py:292\u001B[0m, in \u001B[0;36mnon_max_suppression\u001B[0;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, in_place, rotated)\u001B[0m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    291\u001B[0m     boxes \u001B[38;5;241m=\u001B[39m x[:, :\u001B[38;5;241m4\u001B[39m] \u001B[38;5;241m+\u001B[39m c  \u001B[38;5;66;03m# boxes (offset by class)\u001B[39;00m\n\u001B[0;32m--> 292\u001B[0m     i \u001B[38;5;241m=\u001B[39m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mboxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miou_thres\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# NMS\u001B[39;00m\n\u001B[1;32m    293\u001B[0m i \u001B[38;5;241m=\u001B[39m i[:max_det]  \u001B[38;5;66;03m# limit detections\u001B[39;00m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;66;03m# # Experimental\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;66;03m# merge = False  # use merge-NMS\u001B[39;00m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;66;03m# if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    304\u001B[0m \u001B[38;5;66;03m#     if redundant:\u001B[39;00m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;66;03m#         i = i[iou.sum(1) > 1]  # require redundancy\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/torchvision/ops/boxes.py:40\u001B[0m, in \u001B[0;36mnms\u001B[0;34m(boxes, scores, iou_threshold)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_tracing():\n\u001B[1;32m     39\u001B[0m     _log_api_usage_once(nms)\n\u001B[0;32m---> 40\u001B[0m \u001B[43m_assert_has_ops\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mtorchvision\u001B[38;5;241m.\u001B[39mnms(boxes, scores, iou_threshold)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/torchvision/extension.py:48\u001B[0m, in \u001B[0;36m_assert_has_ops\u001B[0;34m()\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assert_has_ops\u001B[39m():\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _has_ops():\n\u001B[0;32m---> 48\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     49\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load custom C++ ops. This can happen if your PyTorch and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     50\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorchvision versions are incompatible, or if you had errors while compiling \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     51\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorchvision from source. For further information on the compatible versions, check \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     52\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://github.com/pytorch/vision#installation for the compatibility matrix. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     53\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check your PyTorch version with torch.__version__ and your torchvision \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     54\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion with torchvision.__version__ and verify if they are compatible, and if not \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     55\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease reinstall torchvision so that it matches your PyTorch install.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     56\u001B[0m         )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install."
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "ca54c459-c525-49a4-8d19-a6cbbf104081",
   "metadata": {},
   "source": [
    "#### Exercici\n",
    "\n",
    "Carrega una foto emprant la llibreria OpenCV (_cv2_) o la llibreria PIL, fes una predicció i mostra les caixes que envolten els objectes detectats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f32c8-bce1-4cf8-8dc6-f50d33ad3421",
   "metadata": {},
   "source": [
    "### Entrenament\n",
    "\n",
    "Per realitzar l'entrenament s'empra el mètode `train` de la classe `YOLO`. No s'ha de realitzar cap bucle d'entrenament, sino que aquesta funció ens proporciona un nivell d'abstracció superior. És necessari especificar que aquest mètode és altament parametritzable i es fa necessari un estudi del mateix abans d'iniciar un entrenament.\n",
    "\n",
    "Consulta la documentació [enllaç](https://docs.ultralytics.com/modes/train/#key-features-of-train-mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8d7e8-92c5-4286-89ae-3dad9c53a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copiat de la documentació oficial\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLO model\n",
    "model = YOLO(\"yolov5n.pt\") # també es pot carregar un model sense pre entrenar. Es troben en fitxers .yaml\n",
    "\n",
    "# Train the model on the COCO8 example dataset for 100 epochs\n",
    "results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640) #NOTA: Aquí podem entrenar ja que coco8 \"es troba dins ultralytics\"\n",
    "\n",
    "# Run inference\n",
    "results = model(\"path/to/imatge.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c03c97-50b1-4ddf-8773-ad5106f91503",
   "metadata": {},
   "source": [
    "### Segmentació amb YOLO\n",
    "\n",
    "Encara que modificant la versió 5 es poden realitzar tasques de segmentació (veure [enllaç](https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb)), és a partir de la versió 8 que aquesta tasca s'integra dins la xarxa amb l'incorporació d'un nou cap per aquesta tasca.\n",
    "\n",
    "A la documentació podem veure com ja tenim versions de tots els fitxers amb pesos per les diferents tasques: [enllaç](https://docs.ultralytics.com/models/yolov8/#supported-tasks-and-modes).\n",
    "\n",
    "A continuació veurem un exemple de segmentació:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3c55c-49d1-497c-90e4-0351f9500bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv8n model\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "results_seg = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2e79a-b4c6-4fdf-ac1a-dda8be38f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_result = results_seg[0].plot()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img_result);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1ba96-7544-4071-9a2f-f55f7a79977f",
   "metadata": {},
   "source": [
    "#### Exercici\n",
    "\n",
    "En la imatge anterior tenim tant les capses de detecció com les segmentacions:\n",
    "1. Es demana que mostreu només la segmentació de la persona amb major valor de confiança de la xarxa.\n",
    "2. Es demana que mostreu per pantalla les coordenades de la capsa de detecció de la corbata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee53c6-796f-4633-839e-c07a2711e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
