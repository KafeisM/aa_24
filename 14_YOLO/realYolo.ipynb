{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac535cf-32e4-4c01-a6a2-ffc82f3828ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb67056-ac43-41fb-9c4a-ac5168ad822b",
   "metadata": {},
   "source": [
    "# YOLO \n",
    "\n",
    "En aquesta sessi√≥ descobrirem un dels models de l'estat de l'art (SOTA) de l'Aprenentatge Autom√†tic.\n",
    "\n",
    "You Only Look Once: Unified, Real-Time Object Detection. [article](https://arxiv.org/pdf/1506.02640) ; [presentaci√≥](https://www.youtube.com/watch?v=NM6lrxy0bxs&pp=ygUkeW91IG9ubHkgbG9vayBvbmNlIHByZXNlbnRhdGlvbiBjdnBy)\n",
    "\n",
    "YOLO (You Only Look Once) √©s una arquitectura de xarxa neuronal profunda inicialment dissenyada per a la detecci√≥ d'objectes en imatges en temps real. A difer√®ncia d'altres enfocaments que processen les imatges en diverses etapes com per exemple la fam√≠lia R-CNN. YOLO adopta un enfocament unificat: divideix la imatge en una quadr√≠cula i processa cada cel¬∑la simult√†niament per predir les bounding boxes i les classes dels objectes presents. Aquesta integraci√≥ permet assolir una velocitat notable sense comprometre significativament la precisi√≥. A m√©s, gr√†cies al seu disseny optimitzat, YOLO ha estat modificada i adaptada per realitzar m√∫ltiples tasques relacionades amb la visi√≥ per computador. Actualment pot: classificar, detectar, segmentar, seguiment d'objectes a v√≠deo, seguiment dels moviments del cos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499bd7aa-cae8-45ce-8d96-fb3163106f57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### YOLO: Una breu hist√≤ria\n",
    "\n",
    "- **YOLO (You Only Look Once)**, un model popular de detecci√≥ d'objectes i segmentaci√≥ d'imatges, va ser desenvolupat per Joseph Redmon i Ali Farhadi a la Universitat de Washington. Llan√ßat el 2015, YOLO va guanyar r√†pidament popularitat per la seva alta velocitat i precisi√≥.\n",
    "- **YOLOv2**, llan√ßat el 2016, va millorar el model original incorporant _batch normalization_ i  _anchor boxes_.\n",
    "- **YOLOv3**, llan√ßat el 2018, va millorar encara m√©s el rendiment del model mitjan√ßant un _backbone_ m√©s eficient, m√∫ltiples  _anchor boxes_ i agrupaci√≥ de pir√†mides espacials (objectes de m√∫ltiples mides).\n",
    "- **YOLOv4** es va llan√ßar el 2020, introduint innovacions com l'augment de dades emprant mosaics, un nou cap√ßal (_head_) de detecci√≥ i una nova funci√≥ de p√®rdua.\n",
    "- **YOLOv5** va millorar encara m√©s el rendiment del model i va afegir noves funcions com ara l'optimitzaci√≥ d'hiperpar√†metres, el seguiment d'experiments integrat i l'exportaci√≥ autom√†tica a formats d'exportaci√≥ populars. [**Controv√®rsia!!**](https://blog.roboflow.com/yolov4-versus-yolov5/)\n",
    "- **YOLOv6** va ser de codi obert per [Meituan](https://github.com/meituan/YOLOv6) el 2022 i s'utilitza en molts dels robots de lliurament aut√≤noms de la companyia.\n",
    "- **YOLOv7** va afegir tasques addicionals, com ara l'estimaci√≥ de poses (_pose estimation_) al conjunt de dades de punts clau COCO.\n",
    "- **YOLOv8** Es basa en l'√®xit de les versions anteriors, introduint noves funcions i millores per millorar el rendiment, la flexibilitat i l'efici√®ncia. YOLOv8 admet una gamma completa de tasques d'IA de visi√≥, com ara detecci√≥, segmentaci√≥, estimaci√≥ de poses, seguiment i classificaci√≥. Aquesta versatilitat permet als usuaris aprofitar les capacitats de YOLOv8 en diferents aplicacions i dominis. --> Propaganda Ultralytics ^^.\n",
    "- **YOLOv9**, **YOLOv10**, **YOLOv11** : Successives millores en la xarxa, sobretot enfocades a mantenir les seves capacitats redu√Ønt la seva mida.\n",
    "\n",
    "Per comen√ßar i fer les primeres proves es recomana usar la versi√≥ 5, a que t√© un bon equilibri entre dificultat (complicacions d'√∫s) i els resultats que podem obtenir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19f761-f55c-4010-b799-930925b8e90f",
   "metadata": {},
   "source": [
    "## Arquitectura\n",
    "\n",
    "La xarxa t√© 24 capes convolucionals seguides de 2 capes completament connectades. En lloc dels m√≤duls inicials utilitzats per [GoogLeNet](https://arxiv.org/pdf/1409.4842), per reduir el mapes d'activaci√≥ s'utilitzen capes convolucionals 1√ó1 seguides de capes convolucionals de 3√ó3.\n",
    "\n",
    "![YOLO](img/YOLO.png \"YOLO\")\n",
    "\n",
    "\n",
    "### Detecci√≥ unificada\n",
    "\n",
    "A difer√®ncia de les xarxes que provenen de R-CNN a l'article s'explica:\n",
    "\n",
    "> Unifiquem els components separats de la detecci√≥ d'objectes en una √∫nica xarxa neuronal. La nostra xarxa utilitza funcions\n",
    "de tota la imatge per predir cada quadre delimitador. Tamb√© prediu tots els quadres delimitadors (_bounding boxes_) de totes les classes per a una imatge simult√†niament. Aix√≤ vol dir que la nostra xarxa raona globalment sobre la imatge completa i tots els objectes de la imatge.\n",
    "\n",
    "El sistema YOLO divideix la imatge d'entrada en una graella $S√óS$. Si el centre d'un objecte cau dins d'una cel¬∑la de quadr√≠cula, aquesta cel¬∑la de quadr√≠cula s'encarrega de detectar aquest objecte. Cada cel¬∑la de la quadr√≠cula prediu $B$ quadres de delimitaci√≥ i els valors de confian√ßa per a aquestes caixes. \n",
    "\n",
    "Aquestes puntuacions de confian√ßa reflecteixen la confian√ßa que t√© el model que la caixa cont√© un objecte i tamb√© la precisi√≥ que creu que √©s la caixa que prediu. Cada quadre delimitador consta de 5 prediccions: $x, y, w, h$ i la confian√ßa. Les coordenades $(x, y)$ representen el centre del quadre en relaci√≥ amb els l√≠mits de la cel¬∑la de la quadr√≠cula. L'amplada i l'al√ßada es prediuen en relaci√≥ amb tota la imatge. Finalment, la predicci√≥ de confian√ßa representa l'IOU entre la caixa predita i qualsevol caixa de veritat terrestre. Cada cel¬∑la de la quadr√≠cula tamb√© prediu probabilitats de classe condicional $C$.\n",
    "\n",
    "\n",
    "![YOLO](img/YOLO_deteccio.png \"YOLO\")\n",
    "\n",
    "L'arquitectura que hem mostrat anteriorment t√© una capa de sortida de $7x7x30$ degut a que  la sortida √©s correspon amb la seg√ºentv f√≥rmula: $S √ó S √ó (B ‚àó 5 + C)$ en el cas de l'article original: $S=7$, $B=2$ i $C=20$ ja que es va entrenar amb el dataset [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n",
    "\n",
    "Actualment l'arquitectura de la xarxa √©s molt m√©s complexa, i inclou 3 blocs molt diferenciats:\n",
    "\n",
    "- **Columna** \"Backbone\": B√†sicament √©s una xarxa convolucional que extreu caracter√≠stiques. A partir de la versi√≥ 3 creen la seva pr√≤pia xarxa anomenada DarkNet, un model amb connexions residuals que t√© al voltant de 53 capes.\n",
    "- **Coll**: Aquesta part connecta la columna i el/els caps. S'encarrega entre d'altres coses de la detecci√≥ d'objectes a m√∫ltiples escales mitjan√ßant xarxes piramidals que reben informaci√≥ de diversos punts del \"Backbone.\n",
    "- **Cap**: El cap/caps s'encarrega de fer prediccions. En les versions modernes de YOLO s'utilitzen m√∫ltiples m√≤duls de detecci√≥ que prediuen quadres delimitadors, puntuacions d'objectivitat i probabilitats de classe per a cada cel¬∑la de quadr√≠cula del mapa de caracter√≠stiques. A continuaci√≥, aquestes prediccions s'agreguen per obtenir les deteccions finals.\n",
    "\n",
    "Podem veure un exemple d'aquesta complexa arquitectura en el seg√ºent enlla√ß a la documentaci√≥ oficial de la [YOLO v5](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/#1-model-structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3b32c-07ee-4aa3-918d-99807c171dae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## √ös de la xarxa\n",
    "\n",
    "La manera m√©s senzilla d'emprar la xarxa √©s desde la llibreria que la empresa Ultralytics ens ofereix. D'aquesta manera ens √©s molt senzill poder provar les diferents versions de la xarxa i tamb√© realitzar els processos de _fine tunning_ o de _transfer learning_."
   ]
  },
  {
   "cell_type": "code",
   "id": "86c442c9-a9a9-475f-84d4-b44d88c2480b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T15:53:36.365586Z",
     "start_time": "2024-11-27T15:53:32.210072Z"
    }
   },
   "source": "!pip install -U ultralytics",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (8.3.38)\r\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (1.26.4)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (3.9.2)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (4.10.0.84)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (10.4.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (6.0.2)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (2.32.3)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (2.5.1)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (0.15.2a0)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (4.66.5)\r\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (5.9.0)\r\n",
      "Requirement already satisfied: py-cpuinfo in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (9.0.0)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (2.2.2)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from ultralytics) (0.13.2)\r\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\r\n",
      "  Downloading ultralytics_thop-2.0.12-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\r\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\r\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\r\n",
      "Collecting fsspec (from torch>=1.8.0->ultralytics)\r\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting sympy==1.13.1 (from torch>=1.8.0->ultralytics)\r\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/aa24/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\r\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\r\n",
      "Downloading ultralytics_thop-2.0.12-py3-none-any.whl (26 kB)\r\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\r\n",
      "Installing collected packages: sympy, fsspec, ultralytics-thop\r\n",
      "  Attempting uninstall: sympy\r\n",
      "    Found existing installation: sympy 1.13.2\r\n",
      "    Uninstalling sympy-1.13.2:\r\n",
      "      Successfully uninstalled sympy-1.13.2\r\n",
      "Successfully installed fsspec-2024.10.0 sympy-1.13.1 ultralytics-thop-2.0.12\r\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "c6337fa5-33c5-4ed8-92d2-248032570989",
   "metadata": {},
   "source": [
    "Comen√ßarem fent proves amb la YoloV5 que presenta 5 versions diferents, cada una d'aquestes versions t√© una xarxa _backbone_ de mida diferent, a m√©s tenim 2 mides d'entrada d'imatges:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>size<br><sup>(pixels)</sup></th>\n",
    "      <th>mAP<sup>val<br>50-95</sup></th>\n",
    "      <th>mAP<sup>val<br>50</sup></th>\n",
    "      <th>Speed<br><sup>CPU b1<br>(ms)</sup></th>\n",
    "      <th>Speed<br><sup>V100 b1<br>(ms)</sup></th>\n",
    "      <th>Speed<br><sup>V100 b32<br>(ms)</sup></th>\n",
    "      <th>params<br><sup>(M)</sup></th>\n",
    "      <th>FLOPs<br><sup>@640 (B)</sup></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt\" target=\"_blank\">YOLOv5n</a></td>\n",
    "      <td>640</td>\n",
    "      <td>28.0</td>\n",
    "      <td>45.7</td>\n",
    "      <td><strong>45</strong></td>\n",
    "      <td><strong>6.3</strong></td>\n",
    "      <td><strong>0.6</strong></td>\n",
    "      <td><strong>1.9</strong></td>\n",
    "      <td><strong>4.5</strong></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\" target=\"_blank\">YOLOv5s</a></td>\n",
    "      <td>640</td>\n",
    "      <td>37.4</td>\n",
    "      <td>56.8</td>\n",
    "      <td>98</td>\n",
    "      <td>6.4</td>\n",
    "      <td>0.9</td>\n",
    "      <td>7.2</td>\n",
    "      <td>16.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt\" target=\"_blank\">YOLOv5m</a></td>\n",
    "      <td>640</td>\n",
    "      <td>45.4</td>\n",
    "      <td>64.1</td>\n",
    "      <td>224</td>\n",
    "      <td>8.2</td>\n",
    "      <td>1.7</td>\n",
    "      <td>21.2</td>\n",
    "      <td>49.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt\" target=\"_blank\">YOLOv5l</a></td>\n",
    "      <td>640</td>\n",
    "      <td>49.0</td>\n",
    "      <td>67.3</td>\n",
    "      <td>430</td>\n",
    "      <td>10.1</td>\n",
    "      <td>2.7</td>\n",
    "      <td>46.5</td>\n",
    "      <td>109.1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt\" target=\"_blank\">YOLOv5x</a></td>\n",
    "      <td>640</td>\n",
    "      <td>50.7</td>\n",
    "      <td>68.9</td>\n",
    "      <td>766</td>\n",
    "      <td>12.1</td>\n",
    "      <td>4.8</td>\n",
    "      <td>86.7</td>\n",
    "      <td>205.7</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "      <td>&nbsp;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt\" target=\"_blank\">YOLOv5n6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>36.0</td>\n",
    "      <td>54.4</td>\n",
    "      <td>153</td>\n",
    "      <td>8.1</td>\n",
    "      <td>2.1</td>\n",
    "      <td>3.2</td>\n",
    "      <td>4.6</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt\" target=\"_blank\">YOLOv5s6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>44.8</td>\n",
    "      <td>63.7</td>\n",
    "      <td>385</td>\n",
    "      <td>8.2</td>\n",
    "      <td>3.6</td>\n",
    "      <td>12.6</td>\n",
    "      <td>16.8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt\" target=\"_blank\">YOLOv5m6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>51.3</td>\n",
    "      <td>69.3</td>\n",
    "      <td>887</td>\n",
    "      <td>11.1</td>\n",
    "      <td>6.8</td>\n",
    "      <td>35.7</td>\n",
    "      <td>50.0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l6.pt\" target=\"_blank\">YOLOv5l6</a></td>\n",
    "      <td>1280</td>\n",
    "      <td>53.7</td>\n",
    "      <td>71.3</td>\n",
    "      <td>1784</td>\n",
    "      <td>15.8</td>\n",
    "      <td>10.5</td>\n",
    "      <td>76.8</td>\n",
    "      <td>111.4</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><a href=\"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x6.pt\" target=\"_blank\">YOLOv5x6</a><br>+ [TTA]</td>\n",
    "      <td>1280<br>1536</td>\n",
    "      <td>55.0<br><strong>55.8</strong></td>\n",
    "      <td>72.7<br><strong>72.7</strong></td>\n",
    "      <td>3136<br>-</td>\n",
    "      <td>26.2<br>-</td>\n",
    "      <td>19.4<br>-</td>\n",
    "      <td>140.7<br>-</td>\n",
    "      <td>209.8<br>-</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "Explicaci√≥ m√®trica [mAP](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173)\n",
    "\n",
    "Nosaltres comen√ßarem fent proves amb la versi√≥ m√©s petita:"
   ]
  },
  {
   "cell_type": "code",
   "id": "8f0f76f9-1c2c-496c-97b8-b5bbebff4ac2",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T16:02:20.667306Z",
     "start_time": "2024-11-27T16:02:20.641916Z"
    }
   },
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv5n model\n",
    "model = YOLO(\"yolov5n.pt\")\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP üí° Replace 'model=yolov5n.pt' with new 'model=yolov5nu.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "YOLOv5n summary: 262 layers, 2,654,816 parameters, 0 gradients\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(262, 2654816, 0, 0.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "12111eb9-3ae8-4b54-b0bf-8cdd5aacd8df",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T16:02:22.932528Z",
     "start_time": "2024-11-27T16:02:22.929830Z"
    }
   },
   "source": [
    "model;"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "980eac5e-e39f-4ebe-8389-d6869976fbfc",
   "metadata": {},
   "source": [
    "### Infer√®ncia\n",
    "\n",
    "YOLOv5 ha estat entrenat amb el dataset COCO (Common Objects in COntext) [enlla√ß](https://cocodataset.org/#home) que en t√© 80 classes diferents. Fer la infer√®ncia per detecci√≥ √©s molt senzill, √©s suficient amb cridar al model. Aquest ens retorna un objecte de tipus _Results_. [Documentaci√≥](https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Results).\n",
    "\n",
    "Aix√≠, el proc√©s d'infer√®ncia empra l'API d'Ultralytics i es fa enfora de _Pytorch_. "
   ]
  },
  {
   "cell_type": "code",
   "id": "ac12e4c3-36ba-4812-af92-7c543bba6710",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T16:02:24.442304Z",
     "start_time": "2024-11-27T16:02:24.439834Z"
    }
   },
   "source": [
    "# Accepta URL, path, PIL, OpenCV, numpy o una llista\n",
    "img = \"https://hips.hearstapps.com/hmg-prod/images/the-boys-serie-amazon-1565605836.jpg\""
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "16543479-afdc-41e8-888c-197a5233e2cd",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-11-27T16:02:26.012739Z",
     "start_time": "2024-11-27T16:02:25.907743Z"
    }
   },
   "source": [
    "# Inferencia\n",
    "results = model(img)\n",
    "results; # √âs una llista de Results"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https://hips.hearstapps.com/hmg-prod/images/the-boys-serie-amazon-1565605836.jpg locally at the-boys-serie-amazon-1565605836.jpg\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Inferencia\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m results\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/engine/model.py:176\u001B[0m, in \u001B[0;36mModel.__call__\u001B[0;34m(self, source, stream, **kwargs)\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    149\u001B[0m     source: Union[\u001B[38;5;28mstr\u001B[39m, Path, \u001B[38;5;28mint\u001B[39m, Image\u001B[38;5;241m.\u001B[39mImage, \u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m, np\u001B[38;5;241m.\u001B[39mndarray, torch\u001B[38;5;241m.\u001B[39mTensor] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    150\u001B[0m     stream: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    152\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m:\n\u001B[1;32m    153\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001B[39;00m\n\u001B[1;32m    155\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001B[39;00m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 176\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/engine/model.py:554\u001B[0m, in \u001B[0;36mModel.predict\u001B[0;34m(self, source, stream, predictor, **kwargs)\u001B[0m\n\u001B[1;32m    552\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m prompts \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset_prompts\u001B[39m\u001B[38;5;124m\"\u001B[39m):  \u001B[38;5;66;03m# for SAM-type models\u001B[39;00m\n\u001B[1;32m    553\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mset_prompts(prompts)\n\u001B[0;32m--> 554\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor\u001B[38;5;241m.\u001B[39mpredict_cli(source\u001B[38;5;241m=\u001B[39msource) \u001B[38;5;28;01mif\u001B[39;00m is_cli \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredictor\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/engine/predictor.py:173\u001B[0m, in \u001B[0;36mBasePredictor.__call__\u001B[0;34m(self, source, model, stream, *args, **kwargs)\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream_inference(source, model, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/torch/utils/_contextlib.py:36\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m---> 36\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     40\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/engine/predictor.py:266\u001B[0m, in \u001B[0;36mBasePredictor.stream_inference\u001B[0;34m(self, source, model, *args, **kwargs)\u001B[0m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# Postprocess\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m profilers[\u001B[38;5;241m2\u001B[39m]:\n\u001B[0;32m--> 266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpostprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mim0s\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrun_callbacks(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_predict_postprocess_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    269\u001B[0m \u001B[38;5;66;03m# Visualize, save, write results\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/models/yolo/detect/predict.py:25\u001B[0m, in \u001B[0;36mDetectionPredictor.postprocess\u001B[0;34m(self, preds, img, orig_imgs)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpostprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, preds, img, orig_imgs):\n\u001B[1;32m     24\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Post-processes predictions and returns a list of Results objects.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m     preds \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnon_max_suppression\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miou\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43magnostic\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magnostic_nms\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_det\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_det\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(orig_imgs, \u001B[38;5;28mlist\u001B[39m):  \u001B[38;5;66;03m# input images are a torch.Tensor, not a list\u001B[39;00m\n\u001B[1;32m     35\u001B[0m         orig_imgs \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mconvert_torch2numpy_batch(orig_imgs)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/ultralytics/utils/ops.py:292\u001B[0m, in \u001B[0;36mnon_max_suppression\u001B[0;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, max_det, nc, max_time_img, max_nms, max_wh, in_place, rotated)\u001B[0m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    291\u001B[0m     boxes \u001B[38;5;241m=\u001B[39m x[:, :\u001B[38;5;241m4\u001B[39m] \u001B[38;5;241m+\u001B[39m c  \u001B[38;5;66;03m# boxes (offset by class)\u001B[39;00m\n\u001B[0;32m--> 292\u001B[0m     i \u001B[38;5;241m=\u001B[39m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mboxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miou_thres\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# NMS\u001B[39;00m\n\u001B[1;32m    293\u001B[0m i \u001B[38;5;241m=\u001B[39m i[:max_det]  \u001B[38;5;66;03m# limit detections\u001B[39;00m\n\u001B[1;32m    295\u001B[0m \u001B[38;5;66;03m# # Experimental\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;66;03m# merge = False  # use merge-NMS\u001B[39;00m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;66;03m# if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    304\u001B[0m \u001B[38;5;66;03m#     if redundant:\u001B[39;00m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;66;03m#         i = i[iou.sum(1) > 1]  # require redundancy\u001B[39;00m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/torchvision/ops/boxes.py:40\u001B[0m, in \u001B[0;36mnms\u001B[0;34m(boxes, scores, iou_threshold)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_tracing():\n\u001B[1;32m     39\u001B[0m     _log_api_usage_once(nms)\n\u001B[0;32m---> 40\u001B[0m \u001B[43m_assert_has_ops\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mtorchvision\u001B[38;5;241m.\u001B[39mnms(boxes, scores, iou_threshold)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/aa24/lib/python3.10/site-packages/torchvision/extension.py:48\u001B[0m, in \u001B[0;36m_assert_has_ops\u001B[0;34m()\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assert_has_ops\u001B[39m():\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _has_ops():\n\u001B[0;32m---> 48\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m     49\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCouldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt load custom C++ ops. This can happen if your PyTorch and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     50\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorchvision versions are incompatible, or if you had errors while compiling \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     51\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorchvision from source. For further information on the compatible versions, check \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     52\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://github.com/pytorch/vision#installation for the compatibility matrix. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     53\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease check your PyTorch version with torch.__version__ and your torchvision \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     54\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion with torchvision.__version__ and verify if they are compatible, and if not \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     55\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplease reinstall torchvision so that it matches your PyTorch install.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     56\u001B[0m         )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install."
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "ca54c459-c525-49a4-8d19-a6cbbf104081",
   "metadata": {},
   "source": [
    "#### Exercici\n",
    "\n",
    "Carrega una foto emprant la llibreria OpenCV (_cv2_) o la llibreria PIL, fes una predicci√≥ i mostra les caixes que envolten els objectes detectats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f32c8-bce1-4cf8-8dc6-f50d33ad3421",
   "metadata": {},
   "source": [
    "### Entrenament\n",
    "\n",
    "Per realitzar l'entrenament s'empra el m√®tode `train` de la classe `YOLO`. No s'ha de realitzar cap bucle d'entrenament, sino que aquesta funci√≥ ens proporciona un nivell d'abstracci√≥ superior. √âs necessari especificar que aquest m√®tode √©s altament parametritzable i es fa necessari un estudi del mateix abans d'iniciar un entrenament.\n",
    "\n",
    "Consulta la documentaci√≥ [enlla√ß](https://docs.ultralytics.com/modes/train/#key-features-of-train-mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8d7e8-92c5-4286-89ae-3dad9c53a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copiat de la documentaci√≥ oficial\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLO model\n",
    "model = YOLO(\"yolov5n.pt\") # tamb√© es pot carregar un model sense pre entrenar. Es troben en fitxers .yaml\n",
    "\n",
    "# Train the model on the COCO8 example dataset for 100 epochs\n",
    "results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640) #NOTA: Aqu√≠ podem entrenar ja que coco8 \"es troba dins ultralytics\"\n",
    "\n",
    "# Run inference\n",
    "results = model(\"path/to/imatge.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c03c97-50b1-4ddf-8773-ad5106f91503",
   "metadata": {},
   "source": [
    "### Segmentaci√≥ amb YOLO\n",
    "\n",
    "Encara que modificant la versi√≥ 5 es poden realitzar tasques de segmentaci√≥ (veure [enlla√ß](https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb)), √©s a partir de la versi√≥ 8 que aquesta tasca s'integra dins la xarxa amb l'incorporaci√≥ d'un nou cap per aquesta tasca.\n",
    "\n",
    "A la documentaci√≥ podem veure com ja tenim versions de tots els fitxers amb pesos per les diferents tasques: [enlla√ß](https://docs.ultralytics.com/models/yolov8/#supported-tasks-and-modes).\n",
    "\n",
    "A continuaci√≥ veurem un exemple de segmentaci√≥:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3c55c-49d1-497c-90e4-0351f9500bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv8n model\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "\n",
    "results_seg = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2e79a-b4c6-4fdf-ac1a-dda8be38f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_result = results_seg[0].plot()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img_result);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1ba96-7544-4071-9a2f-f55f7a79977f",
   "metadata": {},
   "source": [
    "#### Exercici\n",
    "\n",
    "En la imatge anterior tenim tant les capses de detecci√≥ com les segmentacions:\n",
    "1. Es demana que mostreu nom√©s la segmentaci√≥ de la persona amb major valor de confian√ßa de la xarxa.\n",
    "2. Es demana que mostreu per pantalla les coordenades de la capsa de detecci√≥ de la corbata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee53c6-796f-4633-839e-c07a2711e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
